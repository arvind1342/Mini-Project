{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29128f7a-86eb-4b24-b4a6-8c26a245be73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 5s/step - accuracy: 0.5123 - loss: 0.6944 - val_accuracy: 0.5232 - val_loss: 0.6923\n",
      "Epoch 2/5\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m568s\u001b[0m 8s/step - accuracy: 0.5068 - loss: 0.6933 - val_accuracy: 0.4768 - val_loss: 0.6932\n",
      "Epoch 3/5\n",
      "\u001b[1m27/68\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m3:41\u001b[0m 5s/step - accuracy: 0.4805 - loss: 0.6935"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"Articles.csv\", encoding=\"ISO-8859-1\")\n",
    "\n",
    "# Text Preprocessing Function\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove special characters\n",
    "    return text\n",
    "\n",
    "df[\"Article\"] = df[\"Article\"].apply(clean_text)\n",
    "\n",
    "# Selecting features and labels\n",
    "X = df[\"Article\"]  # Article text as features\n",
    "y = df[\"NewsType\"]  # Target labels\n",
    "\n",
    "# Data Augmentation: Duplicate and slightly modify some entries\n",
    "def augment_text(text):\n",
    "    words = text.split()\n",
    "    if len(words) > 5:\n",
    "        idx = random.randint(0, len(words) - 1)\n",
    "        words[idx] = words[idx][::-1]  # Reverse a random word\n",
    "    return ' '.join(words)\n",
    "\n",
    "df_augmented = df.copy()\n",
    "df_augmented[\"Article\"] = df_augmented[\"Article\"].apply(augment_text)\n",
    "df = pd.concat([df, df_augmented])\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Convert text data into sequences for neural networks\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences to ensure uniform input length\n",
    "max_length = max(len(seq) for seq in X_train_seq)\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_length, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Define Neural Network Model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128, input_length=max_length),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    LSTM(64),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(len(set(y)), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_padded, y_train_encoded, epochs=5, batch_size=32, validation_data=(X_test_padded, y_test_encoded))\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_nn = model.predict(X_test_padded)\n",
    "y_pred_nn_classes = np.argmax(y_pred_nn, axis=1)\n",
    "print(f\"Neural Network Accuracy: {accuracy_score(y_test_encoded, y_pred_nn_classes):.4f}\")\n",
    "\n",
    "# Function to predict class of a given text\n",
    "def predict_news_category(text):\n",
    "    text = clean_text(text)\n",
    "    text_seq = tokenizer.texts_to_sequences([text])\n",
    "    text_padded = pad_sequences(text_seq, maxlen=max_length, padding='post', truncating='post')\n",
    "    prediction = model.predict(text_padded)\n",
    "    predicted_label = label_encoder.inverse_transform([np.argmax(prediction)])\n",
    "    return predicted_label[0]\n",
    "\n",
    "# Example prediction\n",
    "sample_text = \"The star player scores a winning goal in the final match.\"\n",
    "predicted_class_nn = predict_news_category(sample_text)\n",
    "print(f\"Predicted News Category (Neural Network): {predicted_class_nn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b7bf2b-5eed-4931-96ff-cb19c2ce9813",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
